# Azure Data Lakehouse with Databricks & Terraform

This project sets up a complete modular infrastructure for an Azure-based data lakehouse architecture using Terraform and Azure Databricks with Unity Catalog.

## üèóÔ∏è Infrastructure Components

- Resource Group
- Storage Account with containers: `bronze`, `silver`, `gold`
- Azure Databricks Workspace (Premium tier)
- Access Connector for Unity Catalog
- Storage Credential using Azure Managed Identity
- External Locations in Unity Catalog

## üìÅ Folder Structure

```
azure-data-engineer/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ terraform.tfvars
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ resource_group/
‚îÇ   ‚îú‚îÄ‚îÄ storage_account/
‚îÇ   ‚îú‚îÄ‚îÄ databricks_workspace/
‚îÇ   ‚îú‚îÄ‚îÄ databricks_access_connector/
‚îÇ   ‚îú‚îÄ‚îÄ databricks_external_credential/
‚îÇ   ‚îî‚îÄ‚îÄ databricks_external_location/
```

## üöÄ Usage

### 1. Configure Variables

Create a `terraform.tfvars` file:

```hcl
resource_group_name   = "datalake-db"
storage_account_name  = "YOUR_STORAGE_ACCOUNT_NAME"
location              = "eastus"
```

> ‚ö†Ô∏è `storage_account_name` must be globally unique and lowercase.

### 2. Set Azure Subscription

```bash
$env:ARM_SUBSCRIPTION_ID = "<your-subscription-id>"   
```

### 3. Login and Set Subscription

```bash
az login
az account set --subscription "<your-subscription-id>"
```

### 4. Initialize and Apply

```bash
terraform init
terraform plan
terraform apply
```

## üîß Post-Deploy Manual Step

1. Launch the Databricks workspace from the Azure Portal
2. Go to Catalog ‚Üí ‚öôÔ∏è ‚Üí Metastore
3. Add yourself as a **Metastore Admin**
4. Re-run `terraform apply` to finish external credential creation

## üßº Destroy

```bash
terraform destroy
```

> ‚ö†Ô∏è Deleting a Databricks workspace may take 5‚Äì10 minutes.

## ‚úÖ Requirements

- Terraform >= 1.3
- Azure CLI
- Azure subscription with Contributor role

